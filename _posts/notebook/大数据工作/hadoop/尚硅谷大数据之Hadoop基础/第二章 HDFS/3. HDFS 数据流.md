## HDFS 写数据的流程
简单之美 | HDFS 写文件过程分析

HDFS 是一个分布式文件系统，在 HDFS 上写文件的过程与我们平时使用的单机文件系统非常不同，从宏观上来看，在 HDFS 文件系统上创建并写一个文件，流程如下图（来自《Hadoop：The Definitive Guide》一书）所示：  

![](http://shiyanjuncn.b0.upaiyun.com/wp-content/uploads/2014/10/hdfs-write-flow.png)


具体过程描述如下：

1.  Client 调用 DistributedFileSystem 对象的 create 方法，创建一个文件输出流（FSDataOutputStream）对象
2.  通过 DistributedFileSystem 对象与 Hadoop 集群的 NameNode 进行一次 RPC 远程调用，在 HDFS 的 Namespace 中创建一个文件条目（Entry），该条目没有任何的 Block
3.  通过 FSDataOutputStream 对象，向 DataNode 写入数据，数据首先被写入 FSDataOutputStream 对象内部的 Buffer 中，然后数据被分割成一个个 Packet 数据包
4.  以 Packet 最小单位，基于 Socket 连接发送到按特定算法选择的 HDFS 集群中一组 DataNode（正常是 3 个，可能大于等于 1）中的一个节点上，在这组 DataNode 组成的 Pipeline 上依次传输 Packet
5.  这组 DataNode 组成的 Pipeline 反方向上，发送 ack，最终由 Pipeline 中第一个 DataNode 节点将 Pipeline ack 发送给 Client
6.  完成向文件写入数据，Client 在文件输出流（FSDataOutputStream）对象上调用 close 方法，关闭流
7.  调用 DistributedFileSystem 对象的 complete 方法，通知 NameNode 文件写入成功

下面代码使用 Hadoop 的 API 来实现向 HDFS 的文件写入数据，同样也包括创建一个文件和写数据两个主要过程，代码如下所示：
```java
     static String[] contents = new String[] {
          "aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa",
          "bbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbb",
          "cccccccccccccccccccccccccccccccccccccccccccccccccccccccccc",
          "dddddddddddddddddddddddddddddddd",
          "eeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeee",
     };
    
     public static void main(String[] args) {
          String file = "hdfs://h1:8020/data/test/test.log";
        Path path = new Path(file);
        Configuration conf = new Configuration();
        FileSystem fs = null;
        FSDataOutputStream output = null;
        try {
               fs = path.getFileSystem(conf);
               output = fs.create(path); // 创建文件
               for(String line : contents) { // 写入数据
                    output.write(line.getBytes("UTF-8"));
                    output.flush();
               }
          } catch (IOException e) {
               e.printStackTrace();
          } finally {
               try {
                    output.close();
               } catch (IOException e) {
                    e.printStackTrace();
               }
          }
     }
```
结合上面的示例代码，我们先从 fs.create(path); 开始，可以看到 FileSystem 的实现 DistributedFileSystem 中给出了最终返回 FSDataOutputStream 对象的抽象逻辑，代码如下所示：
```
  public FSDataOutputStream create(Path f, FsPermission permission,
    boolean overwrite,
    int bufferSize, short replication, long blockSize,
    Progressable progress) throws IOException {

    statistics.incrementWriteOps(1);
    return new FSDataOutputStream
       (dfs.create(getPathName(f), permission, overwrite, true, replication, blockSize, progress, bufferSize), statistics);
  }
```
上面，DFSClient dfs 的 create 方法中创建了一个 OutputStream 对象，在 DFSClient 的 create 方法：
```
  public OutputStream create(String src,
                             FsPermission permission,
                             boolean overwrite,
                             boolean createParent,
                             short replication,
                             long blockSize,
                             Progressable progress,
                             int buffersize
                             ) throws IOException {
   ... ...
}
```
创建了一个 DFSOutputStream 对象，如下所示：
```
    final DFSOutputStream result = new DFSOutputStream(src, masked,
        overwrite, createParent, replication, blockSize, progress, buffersize,
        conf.getInt("io.bytes.per.checksum", 512));
```
下面，我们从 DFSOutputStream 类开始，说明其内部实现原理。

**DFSOutputStream 内部原理**

打开一个 DFSOutputStream 流，Client 会写数据到流内部的一个缓冲区中，然后数据被分解成多个 Packet，每个 Packet 大小为 64k 字节，每个 Packet 又由一组 chunk 和这组 chunk 对应的 checksum 数据组成，默认 chunk 大小为 512 字节，每个 checksum 是对 512 字节数据计算的校验和数据。  
当 Client 写入的字节流数据达到一个 Packet 的长度，这个 Packet 会被构建出来，然后会被放到队列 dataQueue 中，接着 DataStreamer 线程会不断地从 dataQueue 队列中取出 Packet，发送到复制 Pipeline 中的第一个 DataNode 上，并将该 Packet 从 dataQueue 队列中移到 ackQueue 队列中。ResponseProcessor 线程接收从 Datanode 发送过来的 ack，如果是一个成功的 ack，表示复制 Pipeline 中的所有 Datanode 都已经接收到这个 Packet，ResponseProcessor 线程将 packet 从队列 ackQueue 中删除。  
在发送过程中，如果发生错误，所有未完成的 Packet 都会从 ackQueue 队列中移除掉，然后重新创建一个新的 Pipeline，排除掉出错的那些 DataNode 节点，接着 DataStreamer 线程继续从 dataQueue 队列中发送 Packet。  
下面是 DFSOutputStream 的结构及其原理，如图所示：  

![](http://shiyanjuncn.b0.upaiyun.com/wp-content/uploads/2014/10/hdfs-write-internal.png)


我们从下面 3 个方面来描述内部流程：

-   创建 Packet

Client 写数据时，会将字节流数据缓存到内部的缓冲区中，当长度满足一个 Chunk 大小（512B）时，便会创建一个 Packet 对象，然后向该 Packet 对象中写 Chunk Checksum 校验和数据，以及实际数据块 Chunk Data，校验和数据是基于实际数据块计算得到的。每次满足一个 Chunk 大小时，都会向 Packet 中写上述数据内容，直到达到一个 Packet 对象大小（64K），就会将该 Packet 对象放入到 dataQueue 队列中，等待 DataStreamer 线程取出并发送到 DataNode 节点。

-   发送 Packet

DataStreamer 线程从 dataQueue 队列中取出 Packet 对象，放到 ackQueue 队列中，然后向 DataNode 节点发送这个 Packet 对象所对应的数据。

-   接收 ack

发送一个 Packet 数据包以后，会有一个用来接收 ack 的 ResponseProcessor 线程，如果收到成功的 ack，则表示一个 Packet 发送成功。如果成功，则 ResponseProcessor 线程会将 ackQueue 队列中对应的 Packet 删除。

**DFSOutputStream 初始化**

首先看一下，DFSOutputStream 的初始化过程，构造方法如下所示：

    DFSOutputStream(String src, FsPermission masked, boolean overwrite,
        boolean createParent, short replication, long blockSize, Progressable progress,
        int buffersize, int bytesPerChecksum) throws IOException {
      this(src, blockSize, progress, bytesPerChecksum, replication);
    
      computePacketChunkSize(writePacketSize, bytesPerChecksum); // 默认 writePacketSize=64*1024（即64K），bytesPerChecksum=512（没512个字节计算一个校验和）,
    
      try {
        if (createParent) { // createParent为true表示，如果待创建的文件的父级目录不存在，则自动创建
          namenode.create(src, masked, clientName, overwrite, replication, blockSize);
        } else {
          namenode.create(src, masked, clientName, overwrite, false, replication, blockSize);
        }
      } catch(RemoteException re) {
        throw re.unwrapRemoteException(AccessControlException.class,
                                       FileAlreadyExistsException.class,
                                       FileNotFoundException.class,
                                       NSQuotaExceededException.class,
                                       DSQuotaExceededException.class);
      }
      streamer.start(); // 启动一个DataStreamer线程，用来将写入的字节流打包成packet，然后发送到对应的Datanode节点上
    }
上面computePacketChunkSize方法计算了一个packet的相关参数，我们结合代码来查看，如下所示：
​      int chunkSize = csize + checksum.getChecksumSize();
​      int n = DataNode.PKT_HEADER_LEN + SIZE_OF_INTEGER;
​      chunksPerPacket = Math.max((psize - n + chunkSize-1)/chunkSize, 1);
​      packetSize = n + chunkSize*chunksPerPacket;

我们用默认的参数值替换上面的参数，得到：

      int chunkSize = 512 + 4;
      int n = 21 + 4;
      chunksPerPacket = Math.max((64*1024 - 25 + 516-1)/516, 1);  // 127
      packetSize = 25 + 516*127;

上面对应的参数，说明如下表所示：

![ibyPJ0.png](https://s1.ax1x.com/2018/11/09/ibyPJ0.png)

每个 packet 的字节数（一个 header + 一组 chunk）

在计算好一个 packet 相关的参数以后，调用 create 方法与 Namenode 进行 RPC 请求，请求创建文件：

        if (createParent) { // createParent为true表示，如果待创建的文件的父级目录不存在，则自动创建
          namenode.create(src, masked, clientName, overwrite, replication, blockSize);
        } else {
          namenode.create(src, masked, clientName, overwrite, false, replication, blockSize);
        }

远程调用上面方法，会在 FSNamesystem 中创建对应的文件路径，并初始化与该创建的文件相关的一些信息，如租约（向 Datanode 节点写数据的凭据）。文件在 FSNamesystem 中创建成功，就要初始化并启动一个 DataStreamer 线程，用来向 Datanode 写数据，后面我们详细说明具体处理逻辑。

**Packet 结构与定义**

Client 向 HDFS 写数据，数据会被组装成 Packet，然后发送到 Datanode 节点。Packet 分为两类，一类是实际数据包，另一类是 heatbeat 包。一个 Packet 数据包的组成结构，如图所示：  

![](http://shiyanjuncn.b0.upaiyun.com/wp-content/uploads/2014/10/hdfs-write-packet-structure.png)


上图中，一个 Packet 是由 Header 和 Data 两部分组成，其中 Header 部分包含了一个 Packet 的概要属性信息，如下表所示：
![ibsaxU.png](https://s1.ax1x.com/2018/11/09/ibsaxU.png)
Data 部分是一个 Packet 的实际数据部分，主要包括一个 4 字节校验和（Checksum）与一个 Chunk 部分，Chunk 部分最大为 512 字节。  
在构建一个 Packet 的过程中，首先将字节流数据写入一个 buffer 缓冲区中，也就是从偏移量为 25 的位置（checksumStart）开始写 Packet 数据的 Chunk Checksum 部分，从偏移量为 533 的位置（dataStart）开始写 Packet 数据的 Chunk Data 部分，直到一个 Packet 创建完成为止。如果一个 Packet 的大小未能达到最大长度，也就是上图对应的缓冲区中，Chunk Checksum 与 Chunk Data 之间还保留了一段未被写过的缓冲区位置，这种情况说明，已经在写一个文件的最后一个 Block 的最后一个 Packet。在发送这个 Packet 之前，会检查 Chunksum 与 Chunk Data 之间的缓冲区是否为空白缓冲区（gap），如果有则将 Chunk Data 部分向前移动，使得 Chunk Data 1 与 Chunk Checksum N 相邻，然后才会被发送到 DataNode 节点。  
我们看一下 Packet 对应的 Packet 类定义，定义了如下一些字段：

      ByteBuffer buffer;           // only one of buf and buffer is non-null
      byte[]  buf;
      long    seqno;               // sequencenumber of buffer in block
      long    offsetInBlock;       // 该packet在block中的偏移量
      boolean lastPacketInBlock;   // is this the last packet in block?
      int     numChunks;           // number of chunks currently in packet
      int     maxChunks;           // 一个packet中包含的chunk的个数
      int     dataStart;
      int     dataPos;
      int     checksumStart;
      int     checksumPos; 

Packet 类有一个默认的没有参数的构造方法，它是用来做 heatbeat 的，如下所示：

      Packet() {
        this.lastPacketInBlock = false;
        this.numChunks = 0;
        this.offsetInBlock = 0;
        this.seqno = HEART_BEAT_SEQNO; // 值为-1
    
        buffer = null;
        int packetSize = DataNode.PKT_HEADER_LEN + SIZE_OF_INTEGER; // 21+4=25
        buf = new byte[packetSize];
    
        checksumStart = dataStart = packetSize;
        checksumPos = checksumStart;
        dataPos = dataStart;
        maxChunks = 0;
      }

通过代码可以看到，一个 heatbeat 的内容，实际上只有一个长度为 25 字节的 header 数据。通过 this.seqno = HEART_BEAT_SEQNO; 的值可以判断一个 packet 是否是 heatbeat 包，如果 seqno 为 - 1 表示这是一个 heatbeat 包。

**Client 发送 Packet 数据**

可以 DFSClient 类中看到，发送一个 Packet 之前，首先需要向选定的 DataNode 发送一个 Header 数据包，表明要向 DataNode 写数据，该 Header 的数据结构，如图所示：  

![](http://shiyanjuncn.b0.upaiyun.com/wp-content/uploads/2014/10/hdfs-write-transfer-header.png)


上图显示的是 Client 发送 Packet 到第一个 DataNode 节点的 Header 数据结构，主要包括待发送的 Packet 所在的 Block（先向 NameNode 分配 Block ID 等信息）的相关信息、Pipeline 中另外 2 个 DataNode 的信息、访问令牌（Access Token）和校验和信息，Header 中各个字段及其类型，详见下表：

![ibshse.png](https://s1.ax1x.com/2018/11/09/ibshse.png)
Header 数据包发送成功，Client 会收到一个成功响应码（DataTransferProtocol.OP_STATUS_SUCCESS = 0），接着将 Packet 数据发送到 Pipeline 中第一个 DataNode 上，如下所示：

           Packet one = null;
          one = dataQueue.getFirst(); // regular data packet
          ByteBuffer buf = one.getBuffer();
          // write out data to remote datanode
          blockStream.write(buf.array(), buf.position(), buf.remaining());
    
          if (one.lastPacketInBlock) { // 如果是Block中的最后一个Packet，还要写入一个0标识该Block已经写入完成
              blockStream.writeInt(0); // indicate end-of-block
          }

否则，如果失败，则会与 NameNode 进行 RPC 调用，删除该 Block，并把该 Pipeline 中第一个 DataNode 加入到 excludedNodes 列表中，代码如下所示：

        if (!success) {
          LOG.info("Abandoning " + block);
          namenode.abandonBlock(block, src, clientName);
    
          if (errorIndex < nodes.length) {
            LOG.info("Excluding datanode " + nodes[errorIndex]);
            excludedNodes.add(nodes[errorIndex]);
          }
    
          // Connection failed.  Let's wait a little bit and retry
          retry = true;
        }

**DataNode 端服务组件**

数据最终会发送到 DataNode 节点上，在一个 DataNode 上，数据在各个组件之间流动，流程如下图所示：  

![](http://shiyanjuncn.b0.upaiyun.com/wp-content/uploads/2014/10/hdfs-write-pipeline-single.png)


DataNode 服务中创建一个后台线程 DataXceiverServer，它是一个 SocketServer，用来接收来自 Client（或者 DataNode Pipeline 中的非最后一个 DataNode 节点）的写数据请求，然后在 DataXceiverServer 中将连接过来的 Socket 直接派发给一个独立的后台线程 DataXceiver 进行处理。所以，Client 写数据时连接一个 DataNode Pipeline 的结构，实际流程如图所示：  

![](http://shiyanjuncn.b0.upaiyun.com/wp-content/uploads/2014/10/hdfs-write-pipeline-datanodes.png)


每个 DataNode 服务中的 DataXceiver 后台线程接收到来自前一个节点（Client/DataNode）的 Socket 连接，首先读取 Header 数据：

    Block block = new Block(in.readLong(), dataXceiverServer.estimateBlockSize, in.readLong());
    LOG.info("Receiving " + block + " src: " + remoteAddress + " dest: " + localAddress);
    int pipelineSize = in.readInt(); // num of datanodes in entire pipeline
    boolean isRecovery = in.readBoolean(); // is this part of recovery?
    String client = Text.readString(in); // working on behalf of this client
    boolean hasSrcDataNode = in.readBoolean(); // is src node info present
    if (hasSrcDataNode) {
      srcDataNode = new DatanodeInfo();
      srcDataNode.readFields(in);
    }
    int numTargets = in.readInt();
    if (numTargets < 0) {
      throw new IOException("Mislabelled incoming datastream.");
    }
    DatanodeInfo targets[] = new DatanodeInfo[numTargets];
    for (int i = 0; i < targets.length; i++) {
      DatanodeInfo tmp = new DatanodeInfo();
      tmp.readFields(in);
      targets[i] = tmp;
    }
    Token<BlockTokenIdentifier> accessToken = new Token<BlockTokenIdentifier>();
    accessToken.readFields(in);

上面代码中，读取 Header 的数据，与前一个 Client/DataNode 写入 Header 字段的顺序相对应，不再累述。在完成读取 Header 数据后，当前 DataNode 会首先将 Header 数据再发送到 Pipeline 中下一个 DataNode 结点，当然该 DataNode 肯定不是 Pipeline 中最后一个 DataNode 节点。接着，该 DataNode 会接收来自前一个 Client/DataNode 节点发送的 Packet 数据，接收 Packet 数据的逻辑实际上在 BlockReceiver 中完成，包括将来自前一个 Client/DataNode 节点发送的 Packet 数据写入本地磁盘。在 BlockReceiver 中，首先会将接收到的 Packet 数据发送写入到 Pipeline 中下一个 DataNode 节点，然后再将接收到的数据写入到本地磁盘的 Block 文件中。

**DataNode 持久化 Packet 数据**

在 DataNode 节点的 BlockReceiver 中进行 Packet 数据的持久化，一个 Packet 是一个 Block 中一个数据分组，我们首先看一下，一个 Block 在持久化到磁盘上的物理存储结构，如下图所示：  

![](http://shiyanjuncn.b0.upaiyun.com/wp-content/uploads/2014/10/hdfs-write-block-physical.png)


每个 Block 文件（如上图中 blk_1084013198 文件）都对应一个 meta 文件（如上图中 blk_1084013198_10273532.meta 文件），Block 文件是一个一个 Chunk 的二进制数据（每个 Chunk 的大小是 512 字节），而 meta 文件是与每一个 Chunk 对应的 Checksum 数据，是序列化形式存储。

**写文件过程中 Client/DataNode 与 NameNode 进行 RPC 调用**

Client 在 HDFS 文件系统中写文件过程中，会发生多次与 NameNode 节点进行 RPC 调用来完成写数据相关操作，主要是在如下时机进行 RPC 调用：

-   写文件开始时创建文件：Client 调用 create 在 NameNode 节点的 Namespace 中创建一个标识该文件的条目
-   在 Client 连接 Pipeline 中第一个 DataNode 节点之前，Client 调用 addBlock 分配一个 Block（blkId+DataNode 列表 + 租约）
-   如果与 Pipeline 中第一个 DataNode 节点连接失败，Client 调用 abandonBlock 放弃一个已经分配的 Block
-   一个 Block 已经写入到 DataNode 节点磁盘，Client 调用 fsync 让 NameNode 持久化 Block 的位置信息数据
-   文件写完以后，Client 调用 complete 方法通知 NameNode 写入文件成功
-   DataNode 节点接收到并成功持久化一个 Block 的数据后，DataNode 调用 blockReceived 方法通知 NameNode 已经接收到 Block

具体 RPC 调用的详细过程，可以参考源码。
## HDFS 读数据的流程
[简单之美 | HDFS 读文件过程分析：读取文件的 Block 数据](http://shiyanjun.cn/archives/925.html)

我们可以从 java.io.InputStream 类中看到，抽象出一个 read 方法，用来读取已经打开的 InputStream 实例中的字节，每次调用 read 方法，会读取一个字节数据，该方法抽象定义，如下所示：  
public abstract int read() throws IOException;  
Hadoop 的 DFSClient.DFSInputStream 类实现了该抽象逻辑，如果我们清楚了如何从 HDFS 中读取一个文件的一个 block 的一个字节的原理，更加抽象的顶层只需要迭代即可获取到该文件的全部数据。  
从  [HDFS 读文件过程分析：获取文件对应的 Block 列表](http://shiyanjun.cn/archives/925.html)中，我们已经获取到一个文件对应的 Block 列表信息，打开一个文件，接下来就要读取实际的物理块数据，我们从下面的几个方面来详细说明读取数据的过程。

**Client 从 Datanode 读取文件的一个字节**

下面，我们通过分析 DFSClient.DFSInputStream 中实现的代码，读取 HDFS 上文件的内容。首先从下面的方法开始：
```
    @Override
    public synchronized int read() throws IOException {
      int ret = read( oneByteBuf, 0, 1 );
      return ( ret <= 0 ) ? -1 : (oneByteBuf[0] & 0xff);
    }

上面调用 read(oneByteBuf, 0, 1) 读取一个字节到单字节缓冲区 oneByteBuf 中，具体实现见如下方法：

    @Override
    public synchronized int read(byte buf[], int off, int len) throws IOException {
      checkOpen(); // 检查Client是否正在运行
      if (closed) {
        throw new IOException("Stream closed");
      }
      failures = 0;

      if (pos < getFileLength()) { // getFileLength()获取文件所包含的总字节数，pos表示读取当前文件的第(pos+1)个字节
        int retries = 2;
        while (retries > 0) {
          try {
            if (pos > blockEnd) { // blockEnd表示文件的长度（字节数）
            // 找到第pos个字节数据所在的Datanode（实际根据该字节数据所在的block元数据来定位）
              currentNode = blockSeekTo(pos); 
            }
            int realLen = (int) Math.min((long) len, (blockEnd - pos + 1L));
            int result = readBuffer(buf, off, realLen); // 读取一个字节到缓冲区中
           
            if (result >= 0) {
              pos += result; // 每成功读取result个字节，pos增加result
            } else {
              // got a EOS from reader though we expect more data on it.
              throw new IOException("Unexpected EOS from the reader");
            }
            if (stats != null && result != -1) {
              stats.incrementBytesRead(result);
            }
            return result;
          } catch (ChecksumException ce) {
            throw ce;           
          } catch (IOException e) {
            if (retries == 1) {
              LOG.warn("DFS Read: " + StringUtils.stringifyException(e));
            }
            blockEnd = -1;
            if (currentNode != null) { addToDeadNodes(currentNode); }
            if (--retries == 0) {
              throw e;
            }
          }
        }
      }
      return -1;
    }
```
读取文件数据的一个字节，具体过程如下：

1.  检查流对象是否处于打开状态（前面已经获取到文件对应的 block 列表的元数据，并打开一个 InputStream 对象）
2.  从文件的第一个 block 开始读取，首先需要找到第一个 block 对应的数据块所在的 Datanode，可以从缓存的 block 列表中查询到（如果查找不到，则会与 Namenode 进行一次 RPC 通信请求获取到）
3.  打开一个到该读取的 block 所在 Datanode 节点的流，准备读取 block 数据
4.  建立了到 Datanode 的连接后，读取一个字节数据到字节缓冲区中，返回读取的字节数（1 个字节）

在读取的过程中，以字节为单位，通过判断某个偏移位置的字节属于哪个 block（根据 block 元数据所限定的字节偏移范围），在根据这个 block 去定位某一个 Datanode 节点，这样就可连续地读取一个文件的全部数据（组成文件的、连续的多个 block 数据块）。

**查找待读取的一个字节所在的 Datanode 节点**

上面 public synchronized int read(byte buf[], int off, int len) throws IOException 方法，调用了 blockSeekTo 方法来获取，文件某个字节索引位置的数据所在的 Datanode 节点。其实，很容易就能想到，想要获取到数据所在的 Datanode 节点，一定是从 block 元数据中计算得到，然后根据 Client 缓存的 block 映射列表，找到 block 对应的 Datanode 列表，我们看一下 blockSeekTo 方法的代码实现：
```
    private synchronized DatanodeInfo blockSeekTo(long target) throws IOException {
      ... ...

      DatanodeInfo chosenNode = null;
      int refetchToken = 1; // only need to get a new access token once
      while (true) {
        LocatedBlock targetBlock = getBlockAt(target, true); // 获取字节偏移位置为target的字节数据所在的block元数据对象
        assert (target==this.pos) : "Wrong postion " + pos + " expect " + target;
        long offsetIntoBlock = target - targetBlock.getStartOffset();

        DNAddrPair retval = chooseDataNode(targetBlock); // 选择一个Datanode去读取数据
        chosenNode = retval.info;
        InetSocketAddress targetAddr = retval.addr;

        // 先尝试从本地读取数据，如果数据不在本地，则正常去读取远程的Datanode节点
        Block blk = targetBlock.getBlock();
        Token<BlockTokenIdentifier> accessToken = targetBlock.getBlockToken();
        if (shouldTryShortCircuitRead(targetAddr)) {
          try {
            blockReader = getLocalBlockReader(conf, src, blk, accessToken,
                chosenNode, DFSClient.this.socketTimeout, offsetIntoBlock); // 创建一个用来读取本地数据的BlockReader对象
            return chosenNode;
          } catch (AccessControlException ex) {
            LOG.warn("Short circuit access failed ", ex);
            //Disable short circuit reads
            shortCircuitLocalReads = false;
          } catch (IOException ex) {
            if (refetchToken > 0 && tokenRefetchNeeded(ex, targetAddr)) {
              /* Get a new access token and retry. */
              refetchToken--;
              fetchBlockAt(target);
              continue;
            } else {
              LOG.info("Failed to read " + targetBlock.getBlock()
                  + " on local machine" + StringUtils.stringifyException(ex));
              LOG.info("Try reading via the datanode on " + targetAddr);
            }
          }
        }

        // 本地读取失败，按照更一般的方式去读取远程的Datanode节点来获取数据
        try {
          s = socketFactory.createSocket();
          LOG.debug("Connecting to " + targetAddr);
          NetUtils.connect(s, targetAddr, getRandomLocalInterfaceAddr(), socketTimeout);
          s.setSoTimeout(socketTimeout);
          blockReader = RemoteBlockReader.newBlockReader(s, src, blk.getBlockId(),
              accessToken,
              blk.getGenerationStamp(),
              offsetIntoBlock, blk.getNumBytes() - offsetIntoBlock,
              buffersize, verifyChecksum, clientName); // 创建一个远程的BlockReader对象
          return chosenNode;
        } catch (IOException ex) {
          if (refetchToken > 0 && tokenRefetchNeeded(ex, targetAddr)) {
            refetchToken--;
            fetchBlockAt(target);
          } else {
            LOG.warn("Failed to connect to " + targetAddr
                + ", add to deadNodes and continue" + ex);
            if (LOG.isDebugEnabled()) {
              LOG.debug("Connection failure", ex);
            }
            // Put chosen node into dead list, continue
            addToDeadNodes(chosenNode); // 读取失败，会将选择的Datanode加入到Client的dead node列表，为下次读取选择合适的Datanode读取文件数据提供参考元数据信息
          }
          if (s != null) {
            try {
              s.close();
            } catch (IOException iex) { }                       
          }
          s = null;
        }
      }
    }
```
上面代码中，主要包括如下几个要点：

-   选择合适的 Datanode 节点，提高读取效率

在读取文件的时候，首先会从 Namenode 获取文件对应的 block 列表元数据，返回的 block 列表是按照 Datanode 的网络拓扑结构进行排序过的（本地节点优先，其次是同一机架节点），而且，Client 还维护了一个 dead node 列表，只要此时 bock 对应的 Datanode 列表中节点不出现在 dead node 列表中就会被返回，用来作为读取数据的 Datanode 节点。

-   如果 Client 为集群 Datanode 节点，尝试从本地读取 block

通过调用 chooseDataNode 方法返回一个 Datanode 结点，通过判断，如果该节点地址是本地地址，并且该节点上对应的 block 元数据信息的状态不是正在创建的状态，则满足从本地读取数据块的条件，然后会创建一个 LocalBlockReader 对象，直接从本地读取。在创建 LocalBlockReader 对象的过程中，会先从缓存中查找一个本地 Datanode 相关的 LocalDatanodeInfo 对象，该对象定义了与从本地 Datanode 读取数据的重要信息，以及缓存了待读取 block 对应的本地路径信息，可以从 LocalDatanodeInfo 类定义的属性来说明：
```
    private ClientDatanodeProtocol proxy = null;
    private final Map<Block, BlockLocalPathInfo> cache;
```
如果缓存中存在待读取的 block 的相关信息，可以直接进行读取；否则，会创建一个 proxy 对象，以及计算待读取 block 的路径信息 BlockLocalPathInfo，最后再加入到缓存，为后续可能的读取加速。我们看一下如果没有从缓存中找到 LocalDatanodeInfo 信息（尤其是 BlockLocalPathInfo），则会执行如下逻辑：
```
      // make RPC to local datanode to find local pathnames of blocks
      pathinfo = proxy.getBlockLocalPathInfo(blk, token);
```
上面 proxy 为 ClientDatanodeProtocol 类型，Client 与 Datanode 进行 RPC 通信的协议，RPC 调用 getBlockLocalPathInfo 获取 block 对应的本地路径信息，可以在 Datanode 类中查看具体实现，如下所示：
```
    BlockLocalPathInfo info = data.getBlockLocalPathInfo(block);
```
Datanode 调用 FSDataset（实现接口 FSDatasetInterface）的 getBlockLocalPathInfo，如下所示：
```
  @Override //FSDatasetInterface
  public BlockLocalPathInfo getBlockLocalPathInfo(Block block)
      throws IOException {
    File datafile = getBlockFile(block); // 获取本地block在本地Datanode文件系统中的文件路径
    File metafile = getMetaFile(datafile, block);  // 获取本地block在本地Datanode文件系统中的元数据的文件路径
    BlockLocalPathInfo info = new BlockLocalPathInfo(block, datafile.getAbsolutePath(), metafile.getAbsolutePath());
    return info;
  }
```
接着可以直接去读取该 block 文件（如果需要检查校验和文件，会读取 block 的元数据文件 metafile）：
```
      ... // BlockReaderLocal类的newBlockReader静态方法
      // get a local file system
      File blkfile = new File(pathinfo.getBlockPath());
      dataIn = new FileInputStream(blkfile);

      if (!skipChecksum) { // 如果检查block的校验和
        // get the metadata file
        File metafile = new File(pathinfo.getMetaPath());
        checksumIn = new FileInputStream(metafile);

        // read and handle the common header here. For now just a version
        BlockMetadataHeader header = BlockMetadataHeader.readHeader(new DataInputStream(checksumIn));
        short version = header.getVersion();
        if (version != FSDataset.METADATA_VERSION) {
          LOG.warn("Wrong version (" + version + ") for metadata file for " + blk + " ignoring ...");
        }
        DataChecksum checksum = header.getChecksum();
        localBlockReader = new BlockReaderLocal(conf, file, blk, token, startOffset, length, pathinfo, checksum, true, dataIn, checksumIn);
      } else {
        localBlockReader = new BlockReaderLocal(conf, file, blk, token, startOffset, length, pathinfo, dataIn);
      }
```
在上面代码中，返回了 BlockLocalPathInfo，但是很可能在这个过程中 block 被删除了，在删除 block 的时候，Namenode 会调度指派该 Datanode 删除该 block，恰好在这个时间间隔内 block 对应的 BlockLocalPathInfo 信息已经失效（文件已经被删除），所以上面这段代码再 try 中会抛出异常，并在 catch 中捕获到 IO 异常，会从缓存中再清除掉失效的 block 到 BlockLocalPathInfo 的映射信息。

-   如果 Client 非集群 Datanode 节点，远程读取 block

如果 Client 不是 Datanode 本地节点，则只能跨网络节点远程读取，首先创建 Socket 连接：
```
          s = socketFactory.createSocket();
          LOG.debug("Connecting to " + targetAddr);
          NetUtils.connect(s, targetAddr, getRandomLocalInterfaceAddr(), socketTimeout);
          s.setSoTimeout(socketTimeout);
```
建立 Client 到目标 Datanode（targetAddr）的连接，然后同样也是创建一个远程 BlockReader 对象 RemoteBlockReader 来辅助读取 block 数据。创建 RemoteBlockReader 过程中，首先向目标 Datanode 发送 RPC 请求：
```
      // in and out will be closed when sock is closed (by the caller)
      DataOutputStream out = new DataOutputStream(new BufferedOutputStream(NetUtils.getOutputStream(sock,HdfsConstants.WRITE_TIMEOUT)));

      //write the header.
      out.writeShort( DataTransferProtocol.DATA_TRANSFER_VERSION ); // Client与Datanode之间传输数据的版本号
      out.write( DataTransferProtocol.OP_READ_BLOCK ); // 传输操作类型：读取block
      out.writeLong( blockId ); // block ID
      out.writeLong( genStamp ); // 时间戳信息
      out.writeLong( startOffset ); // block起始偏移量
      out.writeLong( len ); // block长度
      Text.writeString(out, clientName); // 客户端标识
      accessToken.write(out); 
      out.flush();
```
然后获取到 DataInputStream 对象来读取 Datanode 的响应信息：
```
      DataInputStream in = new DataInputStream(
          new BufferedInputStream(NetUtils.getInputStream(sock), bufferSize));
```
最后，返回一个对象 RemoteBlockReader：
```
      return new RemoteBlockReader(file, blockId, in, checksum, verifyChecksum, startOffset, firstChunkOffset, sock);
```
**借助 BlockReader 来读取 block 字节**

我们再回到 blockSeekTo 方法中，待读取 block 所在的 Datanode 信息、BlockReader 信息都已经具备，接着就可以从包含输入流（InputStream）对象的 BlockReader 中读取数据块中一个字节数据：

 int result = readBuffer(buf, off, realLen);

将 block 数据中一个字节读取到 buf 中，如下所示：
```
    private synchronized int readBuffer(byte buf[], int off, int len) throws IOException {
      IOException ioe;
      boolean retryCurrentNode = true;

      while (true) {
        // retry as many times as seekToNewSource allows.
        try {
          return blockReader.read(buf, off, len); // 调用blockReader的read方法读取字节数据到buf中
        } catch ( ChecksumException ce ) {
          LOG.warn("Found Checksum error for " + currentBlock + " from " + currentNode.getName() + " at " + ce.getPos());         
          reportChecksumFailure(src, currentBlock, currentNode);
          ioe = ce;
          retryCurrentNode = false; // 只尝试读取当前选择的Datanode一次，失败的话就会被加入到Client的dead node列表中
        } catch ( IOException e ) {
          if (!retryCurrentNode) {
            LOG.warn("Exception while reading from " + currentBlock + " of " + src + " from " + currentNode + ": " + StringUtils.stringifyException(e));
          }
          ioe = e;
        }
        boolean sourceFound = false;
        if (retryCurrentNode) {
          /* possibly retry the same node so that transient errors don't
           * result in application level failures (e.g. Datanode could have
           * closed the connection because the client is idle for too long).
           */
          sourceFound = seekToBlockSource(pos);
        } else {
          addToDeadNodes(currentNode); // 加入到Client的dead node列表中
          sourceFound = seekToNewSource(pos); // 从当前选择的Datanode上读取数据失败，会再次选择一个Datanode，这里seekToNewSource方法内部调用了blockSeekTo方法去选择一个Datanode
        }
        if (!sourceFound) {
          throw ioe;
        }
        retryCurrentNode = false;
      }
    }
```
通过 BlockReaderLocal 或者 RemoteBlockReader 来读取 block 数据，逻辑非常类似，主要是控制读取字节的偏移量，记录偏移量的状态信息，详细可以查看它们的源码。

**DataNode 节点处理读文件 Block 请求**

我们可以在 DataNode 端看一下，如何处理一个读取 Block 的请求。如果 Client 与 DataNode 不是同一个节点，则为远程读取文件 Block，首先 Client 需要发送一个请求头信息，代码如下所示：
```
      //write the header.
      out.writeShort( DataTransferProtocol.DATA_TRANSFER_VERSION ); // Client与Datanode之间传输数据的版本号
      out.write( DataTransferProtocol.OP_READ_BLOCK ); // 传输操作类型：读取block
      out.writeLong( blockId ); // block ID
      out.writeLong( genStamp ); // 时间戳信息
      out.writeLong( startOffset ); // block起始偏移量
      out.writeLong( len ); // block长度
      Text.writeString(out, clientName); // 客户端标识
      accessToken.write(out); 
      out.flush();
```
DataNode 节点端通过验证数据传输版本号（DataTransferProtocol.DATA_TRANSFER_VERSION）一致以后，会判断传输操作类型，如果是读操作 DataTransferProtocol.OP_READ_BLOCK，则会通过 Client 建立的 Socket 来创建一个 OutputStream 对象，然后通过 BlockSender 向 Client 发送 Block 数据，代码如下所示：
```
      try {
        blockSender = new BlockSender(block, startOffset, length, true, true, false, datanode, clientTraceFmt); // 创建BlockSender对象
      } catch(IOException e) {
        out.writeShort(DataTransferProtocol.OP_STATUS_ERROR);
        throw e;
      }
      out.writeShort(DataTransferProtocol.OP_STATUS_SUCCESS); // 回复一个响应Header信息：成功状态
      long read = blockSender.sendBlock(out, baseStream, null); // 发送请求的Block数据
```
## HDFS HDFS 读文件过程分析：获取文件对应的 Block 列表
[HDFS 读文件过程分析：获取文件对应的 Block 列表](http://shiyanjun.cn/archives/925.html)
在使用 Java 读取一个文件系统中的一个文件时，我们会首先构造一个 DataInputStream 对象，然后就能够从文件中读取数据。对于存储在 HDFS 上的文件，也对应着类似的工具类，但是底层的实现逻辑却是非常不同的。我们先从使用 DFSClient.DFSDataInputStream 类来读取 HDFS 上一个文件的一段代码来看，如下所示：
```
package org.shirdrn.hadoop.hdfs;

import java.io.BufferedReader;
import java.io.IOException;
import java.io.InputStreamReader;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.FSDataInputStream;
import org.apache.hadoop.fs.FileSystem;
import org.apache.hadoop.fs.Path;

public class HdfsFileReader {

     public static void main(String[] args) {
          String file = "hdfs://hadoop-cluster-m:8020/data/logs/basis_user_behavior/201405071237_10_10_1_73.log";
          Path path = new Path(file);
         
          Configuration conf = new Configuration();
          FileSystem fs;
          FSDataInputStream in;
          BufferedReader reader = null;
          try {
               fs = FileSystem.get(conf);
               in = fs.open(path); // 打开文件path，返回一个FSDataInputStream流对象
               reader = new BufferedReader(new InputStreamReader(in));
               String line = null;
               while((line = reader.readLine()) != null) { // 读取文件行内容
                    System.out.println("Record: " + line);
               }
          } catch (IOException e) {
               e.printStackTrace();
          } finally {
               try {
                    if(reader != null) reader.close();
               } catch (IOException e) {
                    e.printStackTrace();
               }
          }
     }

}
```
基于上面代码，我们可以看到，通过一个 FileSystem 对象可以打开一个 Path 文件，返回一个 FSDataInputStream 文件输入流对象，然后从该 FSDataInputStream 对象就能够读取出文件的内容。所以，我们从 FSDataInputStream 入手，详细分析从 HDFS 读取文件内容的过程，在实际地读取物理数据块之前，首先要获取到文件对应的 Block 列表元数据信息，整体流程如下图所示：  

![](http://shiyanjuncn.b0.upaiyun.com/wp-content/uploads/2014/08/hdfs-get-block-locations.png)


下面，详细说明整个流程：

**创建 FSDataInputStream 流对象**

从一个 Path 路径对象，能够获取到一个 FileSystem 对象，然后通过调用 FileSystem 的 open 方法打开一个文件流：
```
  public FSDataInputStream open(Path f) throws IOException {
    return open(f, getConf().getInt("io.file.buffer.size", 4096));
  }
```
由于 FileSystem 是抽象类，将具体的打开操作留给具体子类实现，例如 FTPFileSystem、HarFileSystem、WebHdfsFileSystem 等，不同的文件系统具有不同打开文件的行为，我们以 DistributedFileSystem 为例，open 方法实现，代码如下所示：
```
  public FSDataInputStream open(Path f, int bufferSize) throws IOException {
    statistics.incrementReadOps(1);
    return new DFSClient.DFSDataInputStream(
          dfs.open(getPathName(f), bufferSize, verifyChecksum, statistics));
  }
```
statistics 对象用来收集文件系统操作的统计数据，这里使读取文件操作的计数器加 1。然后创建了一个 DFSClient.DFSDataInputStream 对象，该对象的参数是通过 DFSClient dfs 客户端对象打开一个这个文件从而返回一个 DFSInputStream 对象，下面，我们看 DFSClient 的 open 方法实现，代码如下所示：
```
  public DFSInputStream open(String src, int buffersize, boolean verifyChecksum,
                      FileSystem.Statistics stats) throws IOException {
    checkOpen();
    //    Get block info from namenode
    return new DFSInputStream(src, buffersize, verifyChecksum);
  }
```
checkOpen 方法就是检查一个标志位 clientRunning，表示当前的 dfs 客户端对象是否已经创建并初始化，在 dfs 客户端创建的时候该标志就为 true，表示客户端正在运行状态。我们知道，当客户端 DFSClient 连接到 Namenode 的时候，实际上是创建了一个到 Namenode 的 RPC 连接，Namenode 作为 Server 角色，DFSClient 作为 Client 角色，它们之间建立起 Socket 连接。只有显式调用 DFSClient 的 close 方法时，才会修改 clientRunning 的值为 false，实际上真正地关闭了已经建立的 RPC 连接。  
我们看一下创建 DFSInputStream 的构造方法实现：
```
    DFSInputStream(String src, int buffersize, boolean verifyChecksum) throws IOException {
      this.verifyChecksum = verifyChecksum;
      this.buffersize = buffersize;
      this.src = src;
      prefetchSize = conf.getLong("dfs.read.prefetch.size", prefetchSize);
      openInfo();
    }
```
先设置了几个与读取文件相关的参数值，这里有一个预先读取文件的 Block 字节数的参数 prefetchSize，它的值设置如下：
```
  public static final long DEFAULT_BLOCK_SIZE = DFSConfigKeys.DFS_BLOCK_SIZE_DEFAULT;
  public static final long    DFS_BLOCK_SIZE_DEFAULT = 64*1024*1024;

    defaultBlockSize = conf.getLong("dfs.block.size", DEFAULT_BLOCK_SIZE);
    private long prefetchSize = 10 * defaultBlockSize;
```
这个 prefetchSize 的值默认为 10*64*1024*1024=671088640，也就是说，默认预读取一个文件的 10 个块，即 671088640B=640M，如果想要修改这个值，设置 dfs.block.size 即可覆盖默认值。  
然后调用了 openInfo 方法，从 Namenode 获取到该打开文件的信息，在 openInfo 方法中，具体实现如下所示：
```
    synchronized void openInfo() throws IOException {
      for (int retries = 3; retries > 0; retries--) {
        if (fetchLocatedBlocks()) { 
        // fetch block success. 如果成功获取到待读取文件对应的Block列表，则直接返回
          return;
        } else {
          // Last block location unavailable. When a cluster restarts,
          // DNs may not report immediately. At this time partial block
          // locations will not be available with NN for getting the length.
          // Lets retry a few times to get the length.
          DFSClient.LOG.warn("Last block locations unavailable. "
              + "Datanodes might not have reported blocks completely."
              + " Will retry for " + retries + " times");
          waitFor(4000);
        }
      }
      throw new IOException("Could not obtain the last block locations.");
    }
```
上述代码中，有一个 for 循环用来获取 Block 列表。如果成功获取到待读取文件的 Block 列表，则直接返回，否则，最多执行 3 次等待重试操作（最多花费时间大于 12 秒）。未能成功读取文件的 Block 列表信息，是因为 Namenode 无法获取到文件对应的块列表的信息，当整个集群启动的时候，Datanode 会主动向 NNamenode 上报对应的 Block 信息，只有 Block Report 完成之后，Namenode 就能够知道组成文件的 Block 及其所在 Datanode 列表的信息。openInfo 方法方法中调用了 fetchLocatedBlocks 方法，用来与 Namenode 进行 RPC 通信调用，实际获取对应的 Block 列表，实现代码如下所示：
```
    private boolean fetchLocatedBlocks() throws IOException,
        FileNotFoundException {
      LocatedBlocks newInfo = callGetBlockLocations(namenode, src, 0, prefetchSize);
      if (newInfo == null) {
        throw new FileNotFoundException("File does not exist: " + src);
      }

      if (locatedBlocks != null && !locatedBlocks.isUnderConstruction() && !newInfo.isUnderConstruction()) {
        Iterator<LocatedBlock> oldIter = locatedBlocks.getLocatedBlocks().iterator();
        Iterator<LocatedBlock> newIter = newInfo.getLocatedBlocks().iterator();
        while (oldIter.hasNext() && newIter.hasNext()) {
          if (!oldIter.next().getBlock().equals(newIter.next().getBlock())) {
            throw new IOException("Blocklist for " + src + " has changed!");
          }
        }
      }
      boolean isBlkInfoUpdated = updateBlockInfo(newInfo);
      this.locatedBlocks = newInfo;
      this.currentNode = null;
      return isBlkInfoUpdated;
    }
```
调用 callGetBlockLocations 方法，实际上是根据创建 RPC 连接以后得到的 Namenode 的代理对象，调用 Namenode 来获取到指定文件的 Block 的位置信息（位于哪些 Datanode 节点上）：namenode.getBlockLocations(src, start, length)。调用 callGetBlockLocations 方法返回一个 LocatedBlocks 对象，该对象包含了文件长度信息、List blocks 列表对象，其中 LocatedBlock 包含了一个 Block 的基本信息：
```
  private Block b;
  private long offset;  // offset of the first byte of the block in the file
  private DatanodeInfo[] locs;
  private boolean corrupt;
```
有了这些文件的信息（文件长度、文件包含的 Block 的位置等信息），DFSClient 就能够执行后续读取文件数据的操作了，详细过程我们在后面分析说明。

**通过 Namenode 获取文件信息**

上面，我们提到获取一个文件的基本信息，是通过 Namenode 来得到的，这里详细分析 Namenode 是如何获取到这些文件信息的，实现方法 getBlockLocations 的代码，如下所示：
```
  public LocatedBlocks getBlockLocations(String src, long offset, long length) throws IOException {
    myMetrics.incrNumGetBlockLocations();
    return namesystem.getBlockLocations(getClientMachine(), src, offset, length);
  }
```
可以看到，Namenode 又委托管理 HDFS name 元数据的 FSNamesystem 的 getBlockLocations 方法实现：
```
  LocatedBlocks getBlockLocations(String clientMachine, String src, long offset, long length) throws IOException {
    LocatedBlocks blocks = getBlockLocations(src, offset, length, true, true, true);
    if (blocks != null) {
      //sort the blocks
      // In some deployment cases, cluster is with separation of task tracker
      // and datanode which means client machines will not always be recognized
      // as known data nodes, so here we should try to get node (but not
      // datanode only) for locality based sort.
      Node client = host2DataNodeMap.getDatanodeByHost(clientMachine);
      if (client == null) {
        List<String> hosts = new ArrayList<String> (1);
        hosts.add(clientMachine);
        String rName = dnsToSwitchMapping.resolve(hosts).get(0);
        if (rName != null)
          client = new NodeBase(clientMachine, rName);
      }  

      DFSUtil.StaleComparator comparator = null;
      if (avoidStaleDataNodesForRead) {
        comparator = new DFSUtil.StaleComparator(staleInterval);
      }
      // Note: the last block is also included and sorted
      for (LocatedBlock b : blocks.getLocatedBlocks()) {
        clusterMap.pseudoSortByDistance(client, b.getLocations());
        if (avoidStaleDataNodesForRead) {
          Arrays.sort(b.getLocations(), comparator);
        }
      }
    }
    return blocks;
  }
```
跟踪代码，最终会在下面的方法中实现了，如何获取到待读取文件的 Block 的元数据列表，以及如何取出该文件的各个 Block 的数据，方法实现代码，这里我做了详细的注释，可以参考，如下所示：
```
  private synchronized LocatedBlocks getBlockLocationsInternal(String src,
                                                       long offset,
                                                       long length,
                                                       int nrBlocksToReturn,
                                                       boolean doAccessTime,
                                                       boolean needBlockToken)
                                                       throws IOException {
          INodeFile inode = dir.getFileINode(src);  // 获取到与待读取文件相关的inode数据
          if (inode == null) {
               return null;
          }
          if (doAccessTime && isAccessTimeSupported()) {
               dir.setTimes(src, inode, -1, now(), false);
          }
          Block[] blocks = inode.getBlocks(); // 获取到文件src所包含的Block的元数据列表信息
          if (blocks == null) {
               return null;
          }
          if (blocks.length == 0) { // 获取到文件src的Block数，这里=0，该文件的Block数据还没创建，可能正在创建
               return inode.createLocatedBlocks(new ArrayList<LocatedBlock>(blocks.length));
          }
          List<LocatedBlock> results;
          results = new ArrayList<LocatedBlock>(blocks.length);

          int curBlk = 0; // 当前Block在Block[] blocks数组中的索引位置
          long curPos = 0, blkSize = 0; // curPos表示某个block在文件中的字节偏移量，blkSize为Block的大小（字节数）
          int nrBlocks = (blocks[0].getNumBytes() == 0) ? 0 : blocks.length; // 获取到文件src的Block数，实际上一定>0，但是第一个block大小可能为0，这种情况认为nrBlocks=0
          for (curBlk = 0; curBlk < nrBlocks; curBlk++) {  // 根据前面代码，我们知道offset=0，所以这个循环第一次进来肯定就break出去了（正常的话，blkSize>0，所以我觉得这段代码写的稍微有点晦涩）
               blkSize = blocks[curBlk].getNumBytes();
               assert blkSize > 0 : "Block of size 0";
               if (curPos + blkSize > offset) {
                    break;
               }
               curPos += blkSize;
          }

          if (nrBlocks > 0 && curBlk == nrBlocks) // offset >= end of file, 到这里curBlk=0，如果从文件src的第一个Block的字节数累加计算，知道所有的Block的字节数都累加上了，总字节数仍然<=请求的offset，说明即使到了文件尾部，仍然没有达到offset的值。从前面fetchLocatedBlocks()方法中调用我们知道，offset=0，所以执行该分支表示文件src没有可用的Block数据块可读
               return null;

          long endOff = offset + length; // 

          do {
               // 获取Block所在位置（Datanode节点）
               int numNodes = blocksMap.numNodes(blocks[curBlk]); // 计算文件src中第curBlk个Block存储在哪些Datanode节点上
               int numCorruptNodes = countNodes(blocks[curBlk]).corruptReplicas(); 
               // 计算存储文件src中第curBlk个Block但无法读取该Block的Datanode节点数
               int numCorruptReplicas = corruptReplicas.numCorruptReplicas(blocks[curBlk]); 
               // 计算FSNamesystem在内存中维护的Block=>Datanode映射的列表中，无法读取该Block的Datanode节点数
               if (numCorruptNodes != numCorruptReplicas) {
                    LOG.warn("Inconsistent number of corrupt replicas for "
                              + blocks[curBlk] + "blockMap has " + numCorruptNodes
                              + " but corrupt replicas map has " + numCorruptReplicas);
               }
               DatanodeDescriptor[] machineSet = null;  
               // 下面的if...else用来获取一个Block所在的Datanode节点
               boolean blockCorrupt = false;
               if (inode.isUnderConstruction() && curBlk == blocks.length - 1
                         && blocksMap.numNodes(blocks[curBlk]) == 0) { 
                         // 如果文件正在创建，当前blocks[curBlk]还没有创建成功（即没有可用的Datanode可以提供该Block的服务），仍然返回待创建Block所在的Datanode节点列表。数据块是在Datanode上存储的，只要Datanode完成数据块的存储后，通过heartbeat将数据块的信息上报给Namenode后，这些信息才会存储到blocksMap中
                    // get unfinished block locations
                    INodeFileUnderConstruction cons = (INodeFileUnderConstruction) inode;
                    machineSet = cons.getTargets();
                    blockCorrupt = false;
               } else { // 文件已经创建完成
                    blockCorrupt = (numCorruptNodes == numNodes); 
                    // 是否当前的Block在所有Datanode节点上的副本都坏掉，无法提供服务
                    int numMachineSet = blockCorrupt ? numNodes : (numNodes - numCorruptNodes); 
                    // 如果是，则返回所有Datanode节点，否则，只返回可用的Block副本所在的Datanode节点
                    machineSet = new DatanodeDescriptor[numMachineSet];
                    if (numMachineSet > 0) { 
                    // 获取到当前Block所有副本所在的Datanode节点列表
                         numNodes = 0;
                         for (Iterator<DatanodeDescriptor> it = blocksMap.nodeIterator(blocks[curBlk]); it.hasNext();) {
                              DatanodeDescriptor dn = it.next();
                              boolean replicaCorrupt = corruptReplicas.isReplicaCorrupt(blocks[curBlk], dn);
                              if (blockCorrupt || (!blockCorrupt && !replicaCorrupt))
                                   machineSet[numNodes++] = dn;
                         }
                    }
               }
               LocatedBlock b = new LocatedBlock(blocks[curBlk], machineSet, curPos, blockCorrupt); 
               // 创建一个包含Block的元数据对象、所在Datanode节点列表、起始索引位置（字节数）、健康状况的LocatedBlock对象
               if (isAccessTokenEnabled && needBlockToken) { 
               // 如果启用Block级的令牌（Token）访问，则为当前用户生成读模式的令牌信息，一同封装到返回的LocatedBlock对象中
                    b.setBlockToken(accessTokenHandler.generateToken(b.getBlock(), EnumSet.of(BlockTokenSecretManager.AccessMode.READ)));
               }

               results.add(b); 
               // 收集待返回给读取文件的客户端需要的LocatedBlock列表
               curPos += blocks[curBlk].getNumBytes();
               curBlk++;
          } while (curPos < endOff && curBlk < blocks.length && results.size() < nrBlocksToReturn);

          return inode.createLocatedBlocks(results); 
          // 将收集的LocatedBlock列表数据封装到一个LocatedBlocks对象中返回
     }
```
我们可以看一下，最后的调用 inode.createLocatedBlocks(results) 生成 LocatedBlocks 对象的实现，代码如下所示：
```
  LocatedBlocks createLocatedBlocks(List<LocatedBlock> blocks) {
    return new LocatedBlocks(computeContentSummary().getLength(), blocks, isUnderConstruction()); 
    // 通过ContentSummary对象获取到文件的长度
  }
```
客户端通过 RPC 调用，获取到了文件对应的 Block 以及所在 Datanode 列表的信息，然后就可以根据 LocatedBlocks 来进一步获取到对应的 Block 对应的物理数据块。

**对 Block 列表进行排序**

我们再回到 FSNamesystem 类，调用 getBlockLocationsInternal 方法的 getBlockLocations 方法中，在返回文件 block 列表 LocatedBlocks 之后，会对每一个 Block 所在的 Datanode 进行的一个排序，排序的基本规则有如下 2 点：

-   Client 到 Block 所在的 Datanode 的距离最近，这个是通过网络拓扑关系来进行计算，例如 Client 的网络路径为 / dc1/r1/c1，那么路径为 / dc1/r1/dn1 的 Datanode 就比路径为 / dc1/r2/dn2 的距离小，/dc1/r1/dn1 对应的 Block 就会排在前面
-   从上面一点可以推出，如果 Client 就是某个 Datanode，恰好某个 Block 的 Datanode 列表中包括该 Datanode，则该 Datanode 对应的 Block 排在前面
-   Block 所在的 Datanode 列表中，如果其中某个 Datanode 在指定的时间内没有向 Namenode 发送 heartbeat（默认由常量 DFSConfigKeys.DFS_NAMENODE_STALE_DATANODE_INTERVAL_DEFAULT 定义，默认值为 30s），则该 Datanode 的状态即为 STALE，具有该状态的 Datanode 对应的 Block 排在后面

基于上述规则排序后，Block 列表返回到 Client。

**Client 与 Datanode 交互更新文件 Block 列表**

我们要回到前面分析的 DFSClient.DFSInputStream.fetchLocatedBlocks() 方法中，查看在调用该方法之后，是如何执行实际处理逻辑的：
```
    private boolean fetchLocatedBlocks() throws IOException,
        FileNotFoundException {
      LocatedBlocks newInfo = callGetBlockLocations(namenode, src, 0, prefetchSize); // RPC调用向Namenode获取待读取文件对应的Block及其位置信息LocatedBlocks对象
      if (newInfo == null) {
        throw new FileNotFoundException("File does not exist: " + src);
      }

      if (locatedBlocks != null && !locatedBlocks.isUnderConstruction() && !newInfo.isUnderConstruction()) { // 这里面locatedBlocks!=null是和后面调用updateBlockInfo方法返回的状态有关的
        Iterator<LocatedBlock> oldIter = locatedBlocks.getLocatedBlocks().iterator();
        Iterator<LocatedBlock> newIter = newInfo.getLocatedBlocks().iterator();
        while (oldIter.hasNext() && newIter.hasNext()) { // 检查2次获取到的LocatedBlock列表：第2次得到newInfo包含的Block列表，在第2次得到的locatedBlocks中是否发生变化，如果发生了变化，则不允许读取，抛出异常
          if (!oldIter.next().getBlock().equals(newIter.next().getBlock())) {
            throw new IOException("Blocklist for " + src + " has changed!");
          }
        }
      }
      boolean isBlkInfoUpdated = updateBlockInfo(newInfo);
      this.locatedBlocks = newInfo;
      this.currentNode = null;
      return isBlkInfoUpdated;
    }
```
如果第一次读取该文件时，已经获取到了对应的 block 列表，缓存在客户端；如果客户端第二次又读取了该文件，仍然获取到一个 block 列表对象。在两次读取之间，可能存在原文件完全被重写的情况，所以新得到的 block 列表与原列表完全不同了，存在这种情况，客户端直接抛出 IO 异常，如果原文件对应的 block 列表没有变化，则更新客户端缓存的对应 block 列表信息。  
当集群重启的时候（如果允许安全模式下读文件），或者当一个文件正在创建的时候，Datanode 向 Namenode 进行 Block Report，这个过程中可能 Namenode 还没有完全重建好 Block 到 Datanode 的映射关系信息，所以即使在这种情况下，仍然会返回对应的正在创建的 Block 所在的 Datanode 列表信息，可以从前面 getBlockLocationsInternal 方法中看到，INode 的对应 UnderConstruction 状态为 true。这时，一个 Block 对应的所有副本中的某些可能还在创建过程中。  
上面方法中，调用 updateBlockInfo 来更新文件的 Block 元数据列表信息，对于文件的某些 Block 可能没有创建完成，所以 Namenode 所保存的关于文件的 Block 的的元数据信息可能没有及时更新（Datanode 可能还没有完成 Block 的报告），代码实现如下所示：
```
    private boolean updateBlockInfo(LocatedBlocks newInfo) throws IOException {
      if (!serverSupportsHdfs200 || !newInfo.isUnderConstruction() || !(newInfo.locatedBlockCount() > 0)) { // 如果获取到的newInfo可以读取文件对应的Block信息，则返回true
        return true;
      }

      LocatedBlock last = newInfo.get(newInfo.locatedBlockCount() - 1); // 从Namenode获取文件的最后一个Block的元数据对象LocatedBlock
      boolean lastBlockInFile = (last.getStartOffset() + last.getBlockSize() == newInfo.getFileLength()); 
      if (!lastBlockInFile) { // 如果“文件长度 != 最后一个块起始偏移量 + 最后一个块长度”，说明文件对应Block的元数据信息还没有更新，但是仍然返回给读取文件的该客户端
        return true;
      }
      // 这时，已经确定last是该文件的最后一个bolck，检查最后个block的存储位置信息
      if (last.getLocations().length == 0) {
        return false;
      }
     
      ClientDatanodeProtocol primary = null;
      Block newBlock = null;
      for (int i = 0; i < last.getLocations().length && newBlock == null; i++) { // 根据从Namenode获取到的LocatedBlock last中对应的Datanode列表信息，Client与Datanode建立RPC连接，获取最后一个Block的元数据
        DatanodeInfo datanode = last.getLocations()[i];
        try {
          primary = createClientDatanodeProtocolProxy(datanode, conf, last .getBlock(), last.getBlockToken(), socketTimeout, connectToDnViaHostname);
          newBlock = primary.getBlockInfo(last.getBlock());
        } catch (IOException e) {
          if (e.getMessage().startsWith(
              "java.io.IOException: java.lang.NoSuchMethodException: "
                  + "org.apache.hadoop.hdfs.protocol"
                  + ".ClientDatanodeProtocol.getBlockInfo")) {
            // We're talking to a server that doesn't implement HDFS-200.
            serverSupportsHdfs200 = false;
          } else {
            LOG.info("Failed to get block info from "
                + datanode.getHostName() + " probably does not have "
                + last.getBlock(), e);
          }
        } finally {
          if (primary != null) {
            RPC.stopProxy(primary);
          }
        }
      }
     
      if (newBlock == null) { // Datanode上不存在最后一个Block对应的元数据信息，直接返回
        if (!serverSupportsHdfs200) {
          return true;
        }
        throw new IOException("Failed to get block info from any of the DN in pipeline: " + Arrays.toString(last.getLocations()));
      }
     
      long newBlockSize = newBlock.getNumBytes();
      long delta = newBlockSize - last.getBlockSize();
      // 对于文件的最后一个Block，如果从Namenode获取到的元数据，与从Datanode实际获取到的元数据不同，则以Datanode获取的为准，因为可能Datanode还没有及时将Block的变化信息向Namenode汇报
      last.getBlock().setNumBytes(newBlockSize);
      long newlength = newInfo.getFileLength() + delta;
      newInfo.setFileLength(newlength); // 修改文件Block和位置元数据列表信息
      LOG.debug("DFSClient setting last block " + last + " to length " + newBlockSize + " filesize is now " + newInfo.getFileLength());
      return true;
    }
```
我们看一下，在 updateBlockInfo 方法中，返回 false 的情况：Client 向 Namenode 发起的 RPC 请求，已经获取到了组成该文件的数据块的元数据信息列表，但是，文件的最后一个数据块的存储位置信息无法获取到，说明 Datanode 还没有及时通过 block report 将数据块的存储位置信息报告给 Namenode。通过在 openInfo() 方法中可以看到，获取文件的 block 列表信息有 3 次重试机会，也就是调用 updateBlockInfo 方法返回 false，可以有 12 秒的时间，等待 Datanode 向 Namenode 汇报文件的最后一个块的位置信息，以及 Namenode 更新内存中保存的文件对应的数据块列表元数据信息。  
我们再看一下，在 updateBlockInfo 方法中，返回 true 的情况：

-   文件已经创建完成，文件对应的 block 列表元数据信息可用
-   文件正在创建中，但是当前能够读取到的已经完成的最后一个块（非组成文件的最后一个 block）的元数据信息可用
-   文件正在创建中，文件的最后一个 block 的元数据部分可读：从 Namenode 无法获取到该 block 对应的位置信息，这时 Client 会与 Datanode 直接进行 RPC 通信，获取到该文件最后一个 block 的位置信息

上面 Client 会与 Datanode 直接进行 RPC 通信，获取文件最后一个 block 的元数据，这时可能由于网络问题等等，无法得到文件最后一个 block 的元数据，所以也会返回 true，也就是说，Client 仍然可以读取该文件，只是无法读取到最后一个 block 的数据。  
这样，在 Client 从 Namenode/Datanode 获取到的文件的 Block 列表元数据已经是可用的信息，可以根据这些信息读取到各个 Block 的物理数据块内容了，准确地说，应该是文件处于打开状态了，已经准备好后续进行的读操作了。
<!--stackedit_data:
eyJoaXN0b3J5IjpbMTQ1ODUyMDI0OSwtNjA1NDY3NDcwLC0xND
g3NDY0ODM2LDE3NjI4NTM1NiwxODA3MTE2OTA3LDE4MDcxMTY5
MDcsLTEyOTMzNTQzMjMsMzQwODg0Nzk3LC02MzE2NjEyODQsLT
IwODg3NDY2MTJdfQ==
-->